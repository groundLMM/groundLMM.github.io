<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Self-supervised model for segmenting visual entities and their constituent parts in an open world.">
  <meta name="keywords" content="Large Multimodal Model, Foundation Model, Visual Grounding, Weakly Supervised Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>groundLMM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/button_style.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script defer src="./static/js/brands.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Emerging Pixel Grounding in Large Multimodal Models<br>Without Grounding Supervision</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://shengcao-cao.github.io/">Shengcao Cao</a>,
            </span>
            <span class="author-block">
              <a href="https://lgui.web.illinois.edu/">Liang-Yan Gui</a>,
            </span>
            <span class="author-block">
              <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Illinois Urbana-Champaign</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.08209"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Shengcao-Cao/groundLMM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/Shengcao1006/difflmm-llava-v1.5-7b-lora"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-share-square"></i>
                  </span>
                  <span>Model</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered" style="font-size: 24px; font-weight: bold;">
          <p>
            Want to relate <u>text responses</u> with <u>image regions</u> for your LLaVA-like model?<br>
            Simply check where the model is looking at! No further training needed.
          </p>
        </h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/images/animation.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
</section>

<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current large multimodal models (LMMs) face challenges in grounding,
            which requires the model to relate language components to visual entities.
            Contrary to the common practice that fine-tunes LMMs with additional grounding supervision,
            we find that the grounding ability can in fact emerge in LMMs trained without explicit grounding supervision.
          </p>
          <p>
            To reveal this emerging grounding, we introduce an "attend-and-segment" method
            which leverages attention maps from standard LMMs to perform pixel-level segmentation.
            Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM utilizing a diffusion-based visual encoder,
            as opposed to the standard CLIP visual encoder, and trained with the same weak supervision.
          </p>
          <p>
            Without being constrained by the biases and limited scale of grounding-specific supervision data,
            our approach is more generalizable and scalable.
            We achieve competitive performance on both grounding-specific and general visual question answering benchmarks,
            compared with grounding LMMs and generalist LMMs, respectively.
            Notably, we achieve a 44.2 grounding mask recall on grounded conversation generation without any grounding supervision,
            outperforming the extensively supervised model GLaMM. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">

  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparison</h2>

        <div class="content has-text-justified">
          <img src="./static/images/teaser.png" alt="Qualitative comparison" style="max-width: 100%; height: auto;">
          <p>
            <strong>Grounded conversations with GLaMM vs. our approach, DiffLMM + Attend-and-Segment.</strong>
            As a state-of-the-art grounding LMM, GLaMM is trained to relate text phrases with segmentation masks while generating a response.
            However, due to limitations induced by the grounding supervision, it often fails to precisely follow the human user's instructions
            (e.g., describing the image <i>in detail</i>, answering the correct <i>color</i>).
            Our approach reveals and enhances the <i>grounding ability implicitly learned by LMMs without explicit grounding supervision</i>,
            which leads to visually grounded responses while preserving the general vision-language conversation ability of LMMs.
          </p>
        </div>

        <div class="content has-text-justified">
          <img src="./static/images/gcg.png" alt="Quantitative comparison" style="max-width: 100%; height: auto;">
          <p>
            <strong>Grounded conversation generation (GCG) results.</strong>
            Even without grounding supervision, Attend-and-Segment (a&s in the table) unlocks the implicitly learned grounding ability in LLaVA-1.5,
            <i>outperforming all grounding-specific models on this task</i>.
            DiffLMM further enhances the grounding ability, and leads to stronger grounding performance.
            The higher METEOR scores demonstrate our better preserved conversation ability.
            As a general approach, Attend-and-Segment can be applied on different LMMs (e.g., LLaVA-NeXT and Cambrian-1).
          </p>
        </div>

      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>
        
        <div class="content has-text-justified">
          <img src="./static/images/attention.png" alt="Attend-and-Segment" style="max-width: 100%; height: auto;">
          <p>
            <strong>Meta-architecture of LMMs and the Attend-and-Segment strategy.</strong>
            When generating a new token (e.g., "cat") which requires grounding, we capture the <i>attention</i> between the new token and the input visual tokens.
            Then SAM is employed to refine the processed attention map into a <i>segmentation mask</i> (e.g., cat in the image).
          </p>
        </div>
        
        <div class="content has-text-justified">
          <img src="./static/images/visual.png" alt="DiffLMM" style="max-width: 100%; height: auto;">
          <p>
            <strong>Visual encoding in DiffLMM.</strong>
            We perform one denoising step with the diffusion model (DM), and extract visual features from an intermediate block of the U-Net.
            The implicit captioner produces text-like conditioning and improves the visual features in the U-Net.
            We combine both DM features and CLIP features, and add learnable positional encodings to them.
            The final visual features are projected into the language feature space, and fed into the LLM along with other text tokens. The DM and CLIP visual encoder are frozen.
          </p>
        </div>

      </div>
    </div>

    <div class="columns is-centered"></div>
      <div class="column is-full-width">
        <h2 class="title is-3">More Interesting Examples</h2>
        
        <div class="content has-text-justified">
          <img src="./static/images/qual.png" alt="More examples" style="max-width: 100%; height: auto;">
          <p>
            <strong>Comparison of model responses to challenging visual questions.</strong>
            1) <i>Unusual image contents</i>: The model is requested to analyze the unusual aspect of a given image.
            2) <i>Adversarial questions</i>: The model is asked about something that does not exist in the image.
            3) <i>Rare visual concepts</i>: The image contains objects of less frequent categories.
            4) <i>Shifted image domain</i>: An image from a new domain is given to the model.
          </p>
        </div>

      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{cao2024emerging,
  title={Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision},
  author={Cao, Shengcao and Gui, Liang-Yan and Wang, Yu-Xiong},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
